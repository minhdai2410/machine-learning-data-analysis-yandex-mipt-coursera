{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "\n",
    "#vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#gridsearch\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "#classifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "\n",
    "#words\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_string(*args, **kwargs):\n",
    "    output = io.StringIO()\n",
    "    print(*args, file=output, **kwargs)\n",
    "    contents = output.getvalue()\n",
    "    output.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ –ø–æ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç-–∞–Ω–∞–ª–∏–∑—É\n",
    "–í —ç—Ç–æ–º —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–µ–∫—Å—Ç—É –æ—Ç–∑—ã–≤–∞ –µ–≥–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: 1 - –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è, 0 - –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —É—Å–ª–æ–∂–Ω–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –∑–∞–¥–∞—á–∏, –∑–¥–µ—Å—å –≤–∞–º –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É - –æ–Ω–∞ –µ—Å—Ç—å –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã—Ö –≤–∞–º –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ—á–∫–µ –¥–∞–Ω–æ –ø–æ –æ–¥–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É —Å –∫–ª–∞—Å—Å–æ–º (0 –∏–ª–∏ 1), –∑–∞–ø–∏—Å–∞–Ω–Ω—ã–º —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª —Ç–∞–±—É–ª—è—Ü–∏–∏ –ø–æ—Å–ª–µ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/products_sentiment_train.tsv', sep = '\\t', header = None, names = ['text', 'class'])\n",
    "df_test = pd.read_csv('data/products_sentiment_test.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0          2 . take around 10,000 640x480 pictures .      1\n",
       "1  i downloaded a trial version of computer assoc...      1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
       "3  i dont especially like how music files are uns...      0\n",
       "4  i was using the cheapie pail ... and it worked...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>so , why the small digital elph , rather than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3/4 way through the first disk we played on it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>better for the zen micro is outlook compatibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6 . play gameboy color games on it with goboy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>likewise , i 've heard norton 2004 professiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text\n",
       "0   0  so , why the small digital elph , rather than ...\n",
       "1   1  3/4 way through the first disk we played on it...\n",
       "2   2  better for the zen micro is outlook compatibil...\n",
       "3   3    6 . play gameboy color games on it with goboy .\n",
       "4   4  likewise , i 've heard norton 2004 professiona..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ  2000\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ  500\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ 39512\n",
      "–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ 19.756\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ 3973\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ 1 –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–≤–Ω–æ  0.637\n"
     ]
    }
   ],
   "source": [
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ ', len(df_train))\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ ', len(df_test))\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤', sum(df_train['text'].str.split().apply(len)))\n",
    "print('–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ', sum(df_train['text'].str.split().apply(len))/len(df_train))\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤', TfidfVectorizer().fit_transform(df_train.text).shape[1])\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ 1 –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–≤–Ω–æ ', sum(df_train['class'])/len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ö–∞–∫ –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å\n",
    "–ù–∞–º –¥–∞–ª–∏ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É —Å –∫–ª–∞—Å—Å–∞–º–∏ –∏ —Ç–µ—Å—Ç–æ–≤—É—é –±–µ–∑ –∫–ª–∞—Å—Å–æ–≤. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–æ—Å—Å –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ. –°–æ–≥–ª–∞—Å–Ω–æ –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –º–µ—Ç—Ä–∏–∫–µ (–≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ accuracy) –ø–æ–ª—É—á–∞–µ–º —Å–∞–º—É—é –ª—É—á—à—É—é –º–æ–¥–µ–ª—å, –≤—ã–±—Ä–∞–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é grid search –∏/–∏–ª–∏ –ª–∏—á–Ω—ã—Ö —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏–π/–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ß–µ–º –±–æ–ª—å—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –ø–æ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É, —Ç–µ–º –ª—É—á—à–µ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ - –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–∏—Ö —Å–∞–º–±–∏—Ç–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–∏–¥–Ω–µ–µ—Ç—Å—è. \n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è accuracy. –ü—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–µ –∑–∞–±—ã–≤–∞–π—Ç–µ, —á—Ç–æ –≤–∞—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ—á–Ω–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª—É—á—à–µ, —á–µ–º —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (–≤—Å–µ–≥–¥–∞ 0, –≤—Å–µ–≥–¥–∞ 1, —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –∫–ª–∞—Å—Å–∞).\n",
    "\n",
    "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –º—ã –Ω–µ –∑–Ω–∞–µ–º - –ª–∏—à—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–Ω–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É. –ü–æ—ç—Ç–æ–º—É —ç—Ç–æ –∑–ø–¥–∞–Ω–∏–µ —á–µ–º-—Ç–æ –ø–æ—Ö–æ–∂–µ –≤ –∏–≥—Ä—É –≤ —Å–ª–µ–ø—É—é)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search - –Ω–µ–º–Ω–æ–≥–æ –±–µ–∑–¥—É–º–Ω—ã–π —Å–ø–æ—Å–æ–±, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–æ–∏—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é\n",
    "–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ –≤–æ–∑–º–æ–∂–Ω–æ *—Å–ª–µ–≥–∫–∞* –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è üò¢ –ü–ª—é—Å –∏–∑-–∑–∞ –æ–≥—Ä–æ–º–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –æ—Å–æ–±–æ —É–¥–æ–±–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ç –∫–∞–∫–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞\n",
    "\n",
    "### –°–æ–∑–¥–∞–¥–∏–º –∫–ª–∞—Å—Å—ã-–æ–±–µ—Ä—Ç–∫–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "class Doc2VecModel(BaseEstimator):\n",
    "    def __init__(self, dm=1, vector_size=20, window=5, hs = 1, min_count = 5, epochs = 5, stop_words = None):\n",
    "        self.d2v_model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.dm = dm\n",
    "        self.hs = hs\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        # Initialize model\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, \n",
    "                                window=self.window, \n",
    "                                dm=self.dm,\n",
    "                                hs = self.hs,\n",
    "                                min_count = self.min_count,\n",
    "                                epochs = self.epochs)\n",
    "        # Tag docs\n",
    "        tagged_documents = []\n",
    "        for index, row in raw_documents.iteritems():\n",
    "            tag = '{}_{}'.format(\"type\", index)\n",
    "            tokens = row.split()\n",
    "            if self.stop_words is not None:\n",
    "                tokens = [word for word in tokens if word not in self.stop_words]\n",
    "            tagged_documents.append(TaggedDocument(words=tokens, tags=[tag]))\n",
    "        # Build vocabulary\n",
    "        self.d2v_model.build_vocab(tagged_documents)\n",
    "        # Train model\n",
    "        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.d2v_model.iter)\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        X = []\n",
    "        for index, row in raw_documents.iteritems():\n",
    "            row = row.split()\n",
    "            X.append(self.d2v_model.infer_vector(row))\n",
    "        X = pd.DataFrame(X, index=raw_documents.index)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–∑–¥–∞–¥–∏–º –º–µ–≥–∞ –ø–æ–¥–±–æ—Ä–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∫–∏, —á—Ç–æ–±—ã –∫–æ–º–ø—å—é—Ç–µ—Ä —Å–≥–æ—Ä–µ–ª üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = np.arange(1,11)\n",
    "ngram_range = list(combinations_with_replacement(comb, 2))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect\n",
    "vect_param_grid = {\n",
    "    'vect': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vect__min_df': np.arange(0.0, 1.1, 0.2),\n",
    "    'vect__max_df': np.arange(0.0, 1.1, 0.2),\n",
    "    'vect__ngram_range': ngram_range,\n",
    "    'vect__stop_words': ['english', stopwords.words('english'), get_stop_words('en'), None],\n",
    "    'vect__analyzer': ['word', 'char', 'char_wb']   \n",
    "}\n",
    "hashing_vect_param_grid = {\n",
    "    'vect': [HashingVectorizer()],\n",
    "    'vect__ngram_range': ngram_range,\n",
    "    'vect__stop_words': ['english', stopwords.words('english'), get_stop_words('en'), None],\n",
    "    'vect__analyzer': ['word', 'char', 'char_wb']   \n",
    "}\n",
    "doc2vec_param_grid = {\n",
    "    'vect': [Doc2VecModel()],\n",
    "    'vect__epochs': [1, 3, 5],\n",
    "    'vect__hs': [0, 1],\n",
    "    'vect__vector_size': [1, 10, 50, 100],\n",
    "    'vect__dm':[0,1],\n",
    "    'vect__window': np.arange(1,10,2),\n",
    "    'vect__min_count': [0, 50, 100, 2000],\n",
    "    'vect__stop_words': [stopwords.words('english'), get_stop_words('en'), None]\n",
    "}\n",
    "\n",
    "#clf\n",
    "linear_param_grid = {\n",
    "    'clf' : [LogisticRegression(), SGDClassifier(), RidgeClassifier()]\n",
    "}\n",
    "naive_param_grid = {\n",
    "    'clf': [BernoulliNB(), CategoricalNB(), ComplementNB(), GaussianNB(), MultinomialNB()]\n",
    "}\n",
    "neighbors_param_grid = {\n",
    "    'clf': [KNeighborsClassifier()]\n",
    "}\n",
    "forest_param_grid = {\n",
    "    'clf': [RandomForestClassifier(), GradientBoostingClassifier()]\n",
    "}\n",
    "linear_svc_param_grid = {\n",
    "    'clf':[LinearSVC()],\n",
    "    'clf__loss': ['hinge', 'squared_hinge'],\n",
    "    'clf__multi_class': ['ovr', 'crammer_singer'],\n",
    "    'clf__class_weight': ['balanced', None],\n",
    "    'clf__penalty': ['l1', 'l2']\n",
    "}\n",
    "svc_param_grid = {\n",
    "    'clf':[SVC()],\n",
    "    'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "    'clf__class_weight': ['balanced', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    vect_param_grid,\n",
    "    hashing_vect_param_grid,\n",
    "    doc2vec_param_grid,\n",
    "    \n",
    "    linear_param_grid,\n",
    "    naive_param_grid,\n",
    "    neighbors_param_grid,\n",
    "    forest_param_grid,\n",
    "    linear_svc_param_grid,\n",
    "    svc_param_grid\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–ø—É—Å–∫–∞–µ–º –¥–æ–ª–≥–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–µ—Ç–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11677 candidates, totalling 58385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 436 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1722 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2512 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3186 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3936 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5340 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6408 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7458 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8608 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9858 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11208 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12658 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14208 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15858 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 17608 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=-1)]: Done 19458 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=-1)]: Done 21408 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=-1)]: Done 23684 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 27560 tasks      | elapsed: 45.8min\n",
      "[Parallel(n_jobs=-1)]: Done 30490 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=-1)]: Done 32840 tasks      | elapsed: 58.9min\n",
      "[Parallel(n_jobs=-1)]: Done 35290 tasks      | elapsed: 67.2min\n",
      "[Parallel(n_jobs=-1)]: Done 37840 tasks      | elapsed: 72.6min\n",
      "[Parallel(n_jobs=-1)]: Done 40490 tasks      | elapsed: 76.6min\n",
      "[Parallel(n_jobs=-1)]: Done 43240 tasks      | elapsed: 81.1min\n",
      "[Parallel(n_jobs=-1)]: Done 46090 tasks      | elapsed: 90.2min\n",
      "[Parallel(n_jobs=-1)]: Done 49040 tasks      | elapsed: 102.8min\n",
      "[Parallel(n_jobs=-1)]: Done 52090 tasks      | elapsed: 115.6min\n",
      "[Parallel(n_jobs=-1)]: Done 55240 tasks      | elapsed: 131.0min\n",
      "[Parallel(n_jobs=-1)]: Done 58385 out of 58385 | elapsed: 148.7min finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('vect', Doc2VecModel(vector_size = 1)), ('clf', LinearSVC(class_weight = None))])\n",
    "search = GridSearchCV(pipe, param_grid, verbose = 1, scoring = 'accuracy', n_jobs=-1, cv = 5)\n",
    "search = search.fit(df_train['text'], df_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(search, message = None):\n",
    "    info = re.sub(' +', ' ',print_to_string('vect\\n',search.best_estimator_['vect'], \n",
    "                                            '\\nclf\\n', search.best_estimator_['clf'],\n",
    "                                            '\\nscore\\n', search.best_score_)\n",
    "                 )\n",
    "    print(info)\n",
    "    \n",
    "    message = '' if message is None else message + ' '\n",
    "    name = message + str(search.best_score_) + ' ' + type(search.best_estimator_['vect']).__name__ + ' ' + type(search.best_estimator_['clf']).__name__\n",
    "    \n",
    "    df_test['y'] = pd.DataFrame(search.best_estimator_.predict(df_test['text']))\n",
    "    \n",
    "    df_test[['Id', 'y']].to_csv('results/' + name + '.csv', index=False)\n",
    "    with open('results/' + name + '.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vect\n",
      " TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      " dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      " input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      " min_df=0.0, ngram_range=(1, 10), norm='l2', preprocessor=None,\n",
      " smooth_idf=True, stop_words='english', strip_accents=None,\n",
      " sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      " tokenizer=None, use_idf=True, vocabulary=None) \n",
      "clf\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      " intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      " multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      " verbose=0) \n",
      "score\n",
      " 0.7999999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "* –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ. ngram_range –ø–æ–ª—É—á–∏–ª—Å—è —Å–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º 1-10 —Å–ª–æ–≤, –∞ —á–µ–º –±–æ–ª—å—à–µ —ç—Ç–æ—Ç –¥–∏–∞–ø–æ–∑–æ–Ω, —Ç–µ–º –±–æ–ª—å—à–µ –ø–µ—Ä–µ–æ–±—É–µ—á–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –ø–æ–ª—É—á–∏–ª–∞—Å—å 0.80444.\n",
    "* Doc2Vec –Ω–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è. –ü–æ–ª—É—á–∏–ª–æ—Å—å –¥–æ–±–∏—Ç—å—Å—è –º–∞–ª–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ - 0.666. –≠—Ç–æ–π –º–æ–¥–µ–ª–∏ –±–∞–Ω–∞–ª—å–Ω–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã—Ö - –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å DocVec –Ω–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è —Ç–∞–∫–∏—Ö –º–∞–ª–µ–Ω—å–∫–∏—Ö –≤—ã–±–æ—Ä–æ–∫.\n",
    "* –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–æ–ª–≥–æ –≤ —Å–∏–ª—É —Å–≤–æ–µ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏, –Ω–æ –¥–∞—é—Ç –Ω–µ —Ç–∞–∫–æ–µ —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π –∏ –±—ã—Å—Ç—Ä—ã–π LinearSVC.\n",
    "* TfidfVectorizer –ª—É—á—à–µ CountVectorizer –∑–∞ —Å—á—ë—Ç —Ç–æ–≥–æ, —á—Ç–æ –≤–µ—Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Å–ª–æ–≤–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —á–∞—Å—Ç–æ—Ç–µ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏ –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —á–∞—Å—Ç–æ—Ç–µ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞ –≤–æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ß—Ç–æ –ø–ª–æ—Ö–æ–≥–æ –≤ grid search?\n",
    "* –ü–æ–∏—Å–∫ –ø–æ —Å–µ—Ç–∫–µ –∏—â–µ—Ç —Å–∞–º–æ–µ –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –ø–µ—Ä–µ–±–∏—Ä–∞—è –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–æ–±—É—á–µ–Ω–∏—é (–∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–∂–µ—Ç –æ—Å–ª–∞–±–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –æ–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ). \n",
    "* –ù–µ—É–¥–æ–±–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–Ω–æ—Å—è—Ç –≤–∫–ª–∞–¥ - –∏—Ö –º–Ω–æ–≥–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ \n",
    "–í –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —Å–∞–º–æ–µ –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–∞—ë—Ç TfidfVectorizer, LinearSVC, LogisticRegression - –ø–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–≤–∏—Ç—å –∏–¥–µ—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'vect': [TfidfVectorizer()],\n",
    "    'vect__ngram_range': ngram_range,\n",
    "    'clf':[LinearSVC(),LogisticRegression()],    \n",
    "    'clf__C': np.arange(0.6, 1.7 ,0.2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('vect', TfidfVectorizer(ngram_range = [1,6])), ('clf', LinearSVC(C=1.0))])\n",
    "search = GridSearchCV(pipe, param_grid, verbose = 1, scoring = 'accuracy', n_jobs=-1, cv = 5)\n",
    "search = search.fit(df_train['text'], df_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vect\n",
      " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      " dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      " input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      " min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
      " smooth_idf=True, stop_words=None, strip_accents=None,\n",
      " sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      " tokenizer=None, use_idf=True, vocabulary=None) \n",
      "clf\n",
      " LinearSVC(C=1.4000000000000004, class_weight=None, dual=True,\n",
      " fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n",
      " max_iter=1000, multi_class='ovr', penalty='l2', random_state=None,\n",
      " tol=0.0001, verbose=0) \n",
      "score\n",
      " 0.7915000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —ç—Ç–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–ª–∞ 0.81111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'clf': LinearSVC(C=1.4000000000000004, class_...</td>\n",
       "      <td>0.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'clf': LinearSVC(C=1.4000000000000004, class_...</td>\n",
       "      <td>0.7850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'clf': LinearSVC(C=1.4000000000000004, class_...</td>\n",
       "      <td>0.7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'clf': LinearSVC(C=1.4000000000000004, class_...</td>\n",
       "      <td>0.7780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'clf': LinearSVC(C=1.4000000000000004, class_...</td>\n",
       "      <td>0.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.7230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.7170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.7085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.7035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                params   score\n",
       "0    {'clf': LinearSVC(C=1.4000000000000004, class_...  0.7700\n",
       "1    {'clf': LinearSVC(C=1.4000000000000004, class_...  0.7850\n",
       "2    {'clf': LinearSVC(C=1.4000000000000004, class_...  0.7830\n",
       "3    {'clf': LinearSVC(C=1.4000000000000004, class_...  0.7780\n",
       "4    {'clf': LinearSVC(C=1.4000000000000004, class_...  0.7700\n",
       "..                                                 ...     ...\n",
       "115  {'clf': LogisticRegression(C=1.0, class_weight...  0.7230\n",
       "116  {'clf': LogisticRegression(C=1.0, class_weight...  0.7170\n",
       "117  {'clf': LogisticRegression(C=1.0, class_weight...  0.7085\n",
       "118  {'clf': LogisticRegression(C=1.0, class_weight...  0.7070\n",
       "119  {'clf': LogisticRegression(C=1.0, class_weight...  0.7035\n",
       "\n",
       "[120 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {'params': search.cv_results_['params'],\n",
    "    'score': search.cv_results_['mean_test_score']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∞–º–±–∏—Ç–æ–≤ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∏—Ç—å—Å—è —Ç–æ—á–Ω–æ—Å—Ç–∏ 0.82000 –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
    "* TfidfVectorizer(ngram_range = `[1,6`])\n",
    "* LinearSVC(C = 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –£–≤–µ–ª–∏—á–∏–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
    "–°—Ö–∏—Ç—Ä–∏–º. –£–≤–µ–ª–∏—á–∏–º –≤—ã–±–æ—Ä–∫—É, –¥–æ–±–∞–≤–∏–≤ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É, –∫–æ—Ç–æ—Ä—É—é —Ä–∞–∑–º–µ—Ç–∏–ª–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –≤—ã—à–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2500 entries, 0 to 499\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2500 non-null   object\n",
      " 1   class   2500 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 58.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train_and_test = df_train.append(df_test[['text','y']].rename(columns={\"y\": \"class\"}))\n",
    "df_train_and_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('vect', TfidfVectorizer(ngram_range = [1,6])), ('clf', LinearSVC(C = 1.1))])\n",
    "param_grid = {'vect': [TfidfVectorizer(ngram_range = [1,6])], 'clf': [LinearSVC(C = 1.1)]}\n",
    "search = GridSearchCV(pipe, param_grid, verbose = 1, scoring = 'accuracy', n_jobs=-1, cv = 5)\n",
    "search = search.fit(df_train_and_test['text'], df_train_and_test['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vect\n",
      " TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      " dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      " input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      " min_df=1, ngram_range=[1, 6], norm='l2', preprocessor=None,\n",
      " smooth_idf=True, stop_words=None, strip_accents=None,\n",
      " sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      " tokenizer=None, use_idf=True, vocabulary=None) \n",
      "clf\n",
      " LinearSVC(C=1.1, class_weight=None, dual=True, fit_intercept=True,\n",
      " intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      " multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      " verbose=0) \n",
      "score\n",
      " 0.8132000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(search, 'df_train_and_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Å—Ç–∞–ª–∞ —Ö—É–∂–µ - 0.8066. –¢–æ –µ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ –æ—á–µ–Ω—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∫ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥\n",
    "* –≤—ã–±–æ—Ä–∫–∞ –º–∞–ª–µ–Ω—å–∫–∞—è, –∏–∑-–∑–∞ —á–µ–≥–æ –ø–æ–ª—É—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –±–æ–ª—å—à–µ 80% —Å–ª–æ–∂–Ω–æ–≤–∞—Ç–æ.\n",
    "* —Ç–µ–∫—Å—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –æ—à–∏–±–∫–∏ –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ò–Ω–æ–≥–¥–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã. –°–∞–º–∞ –≤—ã–±–æ—Ä–∫–∞ –Ω–µ —Ç–∞–∫–∞—è —Ö–æ—Ä–æ—à–∞—è, –∫–∞–∫ —Ö–æ—Ç–µ–ª–æ—Å—å –±—ã.\n",
    "* –¥–∞–Ω–Ω–æ–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–µ –ø–æ—Ö–æ–¥–∏–ª–æ –Ω–∞ –∏–≥—Ä—É –≤—Å–ª–µ–ø—É—é - –º—ã —Å—Ç–∞—Ä–∞–µ–º—Å—è –¥–æ–±–∏—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ, –∑–Ω–∞—è –ª–∏—à—å –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—ä—ë–º –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –∏–Ω–æ–≥–¥–∞ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–∞–≤–Ω—è–ª–æ—Å—å –∫–∞—á–µ—Å—Ç–≤—É –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π - –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –¥–æ—Ö–æ–¥–∏–ª–∏ –¥–æ 2-4% –≤ —Ç—É –∏–ª–∏ –∏–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É. –ù–æ –≤—Å—ë —Ä–∞–≤–Ω–æ —á–µ–º –±–æ–ª—å—à–µ –±—ã–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ, —Ç–µ–º –±–æ–ª—å—à–µ –±—ã–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π (—Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏ –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π).\n",
    "* –ª—É—á—à–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º –Ω–∞ —Ç–∞–∫–∏–º –æ–±—ä—ë–º–∞—Ö —è–≤–ª—è–µ—Ç—Å—è TFIDFVectorizer + LogisticRegression –∏–ª–∏ LineraSVC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
